{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg Classification Step 2a: Model Training with Single GPU\n",
    "The following code includes demonstration for:\n",
    "- get data from ``feature store``\n",
    "- training with ``TFRecord`` on a single GPU\n",
    "- hyperparameter optimization with ``maggy``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>51</td><td>application_1574692443370_0052</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-23-184.eu-north-1.compute.internal:8088/proxy/application_1574692443370_0052/\">Link</a></td><td><a target=\"_blank\" href=\"http://localhost:8042/node/containerlogs/container_e03_1574692443370_0052_01_000001/ExtremeEarth__meb10000\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "Version of TensorFlow is 1.14.0"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Version of TensorFlow is {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running maggy on Hopsworks."
     ]
    }
   ],
   "source": [
    "from hops import featurestore\n",
    "from hops import tensorboard\n",
    "from maggy import experiment\n",
    "from maggy.callbacks import KerasBatchEnd\n",
    "from maggy.callbacks import KerasEpochEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(tfrecord_path, name_list):\n",
    "    dataset_dir = featurestore.get_training_dataset_path(tfrecord_path)\n",
    "    input_files = tf.gfile.Glob(dataset_dir + \"/part-r-*\")\n",
    "    dataset = tf.data.TFRecordDataset(input_files)\n",
    "    # 'tf_record_schema' is needed because we need to parse a single example from all the TFRecords we have\n",
    "    tf_record_schema = featurestore.get_training_dataset_tf_record_schema(tfrecord_path)\n",
    "\n",
    "    def decode(example_proto):\n",
    "        example = tf.parse_single_example(example_proto, tf_record_schema)\n",
    "        x = tf.stack([example[name_list[0]], example[name_list[1]], example[name_list[2]]], axis=1)\n",
    "        x = tf.reshape(x, [75, 75, 3])\n",
    "        y = [tf.cast(example[name_list[3]], tf.float32)]\n",
    "        return x,y\n",
    "    \n",
    "    dataset = dataset.map(decode).shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat(NUM_EPOCHS)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(kernel, pool, dropout):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    #Conv Layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size=(kernel, kernel), activation='relu', input_shape=INPUT_SHAPE))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(pool, pool), strides=(2,2)))\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    #Conv Layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size=(kernel, kernel), activation='relu' ))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(pool, pool), strides=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "    \n",
    "    #Conv Layer 3\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 4\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Flatten the data for upcoming dense layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    #Dense Layers\n",
    "    model.add(tf.keras.layers.Dense(512))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Dense Layer 2\n",
    "    model.add(tf.keras.layers.Dense(256))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Sigmoid Layer\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "### maggy: hyperparameters as arguments and including the reporter\n",
    "#########\n",
    "def train_fn(kernel, pool, dropout, reporter):\n",
    "    train_tfrecord_path = \"train_tfrecords_iceberg_classification_dataset\"\n",
    "    train_name_list = [\"band_1\", \"band_2\", \"band_avg\", \"is_iceberg\"]\n",
    "    train_dataset = create_tf_dataset(train_tfrecord_path, train_name_list)\n",
    "    \n",
    "    test_tfrecord_path = \"test_tfrecords_iceberg_classification_dataset\"\n",
    "    test_name_list = [\"band_1\", \"band_2\", \"band_avg\", \"is_iceberg\"]\n",
    "    test_dataset = create_tf_dataset(test_tfrecord_path, test_name_list)\n",
    "    \n",
    "    \n",
    "    model = create_model(kernel, pool, dropout)\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(LEARNING_RATE), loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "\n",
    "    #########\n",
    "    ### maggy: REPORTER API through keras callback\n",
    "    #########\n",
    "\n",
    "    callbacks = [KerasBatchEnd(reporter, metric='acc')] # sina\n",
    "\n",
    "    \n",
    "    # fit the model\n",
    "    history = model.fit(train_dataset, epochs=NUM_EPOCHS, steps_per_epoch=5, callbacks=callbacks)\n",
    "    \n",
    "    \n",
    "    test_score = model.evaluate(test_dataset, verbose=1) # sina\n",
    "    print('Test loss:', test_score[0]) # sina\n",
    "    print('Test accuracy:', test_score[1]) # sina\n",
    "    #########\n",
    "    ### maggy: return the metric to be optimized, test accuracy in this case\n",
    "    #########\n",
    "    \n",
    "    return test_score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter for TFRecords\n",
    "NUM_EPOCHS = 2 # sina\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "# Hyperparameter for learning rate\n",
    "LEARNING_RATE = 0.001\n",
    "# Input shape of the model\n",
    "INPUT_SHAPE= (75, 75, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the search space for hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter added: kernel\n",
      "Hyperparameter added: pool\n",
      "Hyperparameter added: dropout"
     ]
    }
   ],
   "source": [
    "from maggy import Searchspace\n",
    "\n",
    "# The searchspace can be instantiated with parameters\n",
    "sp = Searchspace()\n",
    "\n",
    "# Or additional parameters can be added one by one\n",
    "sp.add('kernel', ('INTEGER', [3, 4]))\n",
    "sp.add('pool', ('INTEGER', [2, 3]))\n",
    "sp.add('dropout', ('DOUBLE', [0.10, 0.50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch the hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175f682be73e4d75b65d6b80b9a68bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Maggy experiment', max=10, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Epoch 1/2\n",
      "0: Epoch 2/2\n",
      "0: Test loss: 0.6870104521512985\n",
      "0: Test accuracy: 0.5375\n",
      "0: Epoch 1/2\n",
      "0: Epoch 2/2\n",
      "0: Test loss: 0.6976054519414902\n",
      "0: Test accuracy: 0.4625\n",
      "0: Epoch 1/2\n",
      "0: Epoch 2/2\n",
      "0: Test loss: 0.6794597119092941\n",
      "0: Test accuracy: 0.5375\n",
      "0: Epoch 1/2\n",
      "0: Epoch 2/2\n",
      "0: Test loss: 0.6886839509010315\n",
      "0: Test accuracy: 0.528125\n",
      "0: Epoch 1/2\n",
      "0: Epoch 2/2\n",
      "0: Test loss: 0.649113941192627\n",
      "0: Test accuracy: 0.66875\n",
      "0: Epoch 1/2\n",
      "0: Epoch 2/2\n",
      "0: Test loss: 0.6886781752109528\n",
      "0: Test accuracy: 0.50625\n",
      "0: Epoch 1/2\n",
      "0: Epoch 2/2\n",
      "0: Test loss: 0.6886705309152603\n",
      "0: Test accuracy: 0.5375\n",
      "0: Epoch 1/2\n",
      "0: Epoch 2/2\n",
      "0: Test loss: 0.7179129153490067\n",
      "0: Test accuracy: 0.4625\n",
      "0: Epoch 1/2\n",
      "0: Epoch 2/2\n",
      "0: Test loss: 0.6826905578374862\n",
      "0: Test accuracy: 0.5375\n",
      "0: Epoch 1/2\n",
      "0: Epoch 2/2\n",
      "0: Test loss: 0.6882031589746476\n",
      "0: Test accuracy: 0.61875\n",
      "\n",
      "------ RandomSearch Results ------ direction(max) \n",
      "BEST combination {\"kernel\": 4, \"pool\": 3, \"dropout\": 0.2209323739211756} -- metric 0.66875\n",
      "WORST combination {\"kernel\": 4, \"pool\": 3, \"dropout\": 0.21076678446151306} -- metric 0.4625\n",
      "AVERAGE metric -- 0.5396875083446503\n",
      "EARLY STOPPED Trials -- 0\n",
      "Total job time 8 minutes, 22 seconds\n"
     ]
    }
   ],
   "source": [
    "result = experiment.lagom(map_fun=train_fn, \n",
    "                           searchspace=sp, \n",
    "                           optimizer='randomsearch', \n",
    "                           direction='max', \n",
    "                           num_trials=10, \n",
    "                           name='Iceberg_Classification_Maggy', \n",
    "                           hb_interval=5,\n",
    "                           es_interval=300,\n",
    "                           es_min=5\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Step2a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}