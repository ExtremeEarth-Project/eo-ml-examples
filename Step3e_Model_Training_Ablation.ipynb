{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg Classification Step 2: Model Training with Single GPU\n",
    "The following code includes demonstration for:\n",
    "- get data from ``feature store``\n",
    "- training with ``TFRecord`` on a single GPU\n",
    "- ablation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>61</td><td>application_1574692443370_0062</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-23-184.eu-north-1.compute.internal:8088/proxy/application_1574692443370_0062/\">Link</a></td><td><a target=\"_blank\" href=\"http://localhost:8042/node/containerlogs/container_e03_1574692443370_0062_01_000001/ExtremeEarth__meb10000\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "Version of TensorFlow is 1.14.0"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Version of TensorFlow is {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running maggy on Hopsworks."
     ]
    }
   ],
   "source": [
    "from hops import featurestore\n",
    "from hops import tensorboard\n",
    "import maggy\n",
    "from maggy import experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(tfrecord_path, name_list):\n",
    "    dataset_dir = featurestore.get_training_dataset_path(tfrecord_path)\n",
    "    input_files = tf.gfile.Glob(dataset_dir + \"/part-r-*\")\n",
    "    dataset = tf.data.TFRecordDataset(input_files)\n",
    "    # 'tf_record_schema' is needed because we need to parse a single example from all the TFRecords we have\n",
    "    tf_record_schema = featurestore.get_training_dataset_tf_record_schema(tfrecord_path)\n",
    "\n",
    "    def decode(example_proto):\n",
    "        example = tf.parse_single_example(example_proto, tf_record_schema)\n",
    "        x = tf.stack([example[name_list[0]], example[name_list[1]], example[name_list[2]]], axis=1)\n",
    "        x = tf.reshape(x, [75, 75, 3])\n",
    "        y = [tf.cast(example[name_list[3]], tf.float32)]\n",
    "        return x,y\n",
    "    \n",
    "    dataset = dataset.map(decode).shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat(NUM_EPOCHS)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    #Conv Layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=INPUT_SHAPE, name='my_conv_1'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2), name='my_maxpool_1'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2, name='my_dropout_1'))\n",
    "\n",
    "    #Conv Layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu', name='my_conv_2'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='my_maxpool_2'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2, name='my_dropout_2'))\n",
    "\n",
    "    #Conv Layer 3\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu', name='my_conv_3'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='my_maxpool_3'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2, name='my_dropout_3'))\n",
    "\n",
    "    #Conv Layer 4\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', name='my_conv_4'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='my_maxpool_4'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2, name='my_dropout_4'))\n",
    "\n",
    "    #Flatten the data for upcoming dense layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    #Dense Layers\n",
    "    model.add(tf.keras.layers.Dense(512))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Dense Layer 2\n",
    "    model.add(tf.keras.layers.Dense(256))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Sigmoid Layer\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter for TFRecords\n",
    "NUM_EPOCHS = 50 # sina\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "# # Hyperparameter for learning rate\n",
    "# LEARNING_RATE = 0.001\n",
    "# Input shape of the model\n",
    "INPUT_SHAPE= (75, 75, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Study Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AblationStudy instance\n",
    "from maggy.ablation import AblationStudy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iceberg_ablation = AblationStudy('iceberg', training_dataset_version=1, label_name=\"is_iceberg\",\n",
    "                                 dataset_generator=create_tf_dataset) # sina\n",
    "# do we need to specify label_name for model ablation study?\n",
    "\n",
    "# pass the model generator function to ablation study\n",
    "# set the base model generator\n",
    "iceberg_ablation.model.set_base_model_generator(create_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Included layer groups are: \n",
      "\n",
      "---- All layers prefixed \"my_dropout\"\n",
      "---- All layers prefixed \"my_conv\"\n",
      "---- All layers prefixed \"my_maxpool\""
     ]
    }
   ],
   "source": [
    "# # add layers to the ablation study\n",
    "# iceberg_ablation.model.layers.include(['my_conv_1', 'my_conv_2', 'my_conv_3', 'my_conv_4'])\n",
    "# iceberg_ablation.model.layers.include(['my_maxpool_1', 'my_maxpool_2', 'my_maxpool_3', 'my_maxpool_4'])\n",
    "# iceberg_ablation.model.layers.include(['my_dropout_1', 'my_dropout_2', 'my_dropout_3', 'my_dropout_4'])\n",
    "\n",
    "# iceberg_ablation.model.layers.print_all()\n",
    "\n",
    "# add a layer group using a prefix\n",
    "\n",
    "iceberg_ablation.model.layers.include_groups(prefix='my_conv')\n",
    "iceberg_ablation.model.layers.include_groups(prefix='my_maxpool')\n",
    "iceberg_ablation.model.layers.include_groups(prefix='my_dropout')\n",
    "\n",
    "iceberg_ablation.model.layers.print_all_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training_dataset_name': 'iceberg', 'training_dataset_version': 1, 'label_name': 'is_iceberg', 'included_features': [], 'included_layers': [], 'custom_dataset_generator': True}"
     ]
    }
   ],
   "source": [
    "iceberg_ablation.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training function for ablation study\n",
    "def ablation_training_fn(dataset_function, model_function):\n",
    "    import tensorflow as tf\n",
    "    epochs = 50 # sina\n",
    "    batch_size = 10\n",
    "    \n",
    "    tfrecord_path = \"train_tfrecords_iceberg_classification_dataset\" # sina\n",
    "    name_list = [\"band_1\", \"band_2\", \"band_avg\", \"is_iceberg\"] # sina\n",
    "    tf_dataset = dataset_function(tfrecord_path, name_list) # sina\n",
    "    \n",
    "    model = create_model()\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(tf_dataset, epochs=epochs, steps_per_epoch=50, verbose=1) # sina\n",
    "    return float(history.history['acc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4522ae87f74119b6a373c5ff7c3d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Maggy experiment', max=4, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Epoch 1/50\n",
      "0: Epoch 2/50\n",
      "0: Epoch 3/50\n",
      "0: Epoch 4/50\n",
      "0: Epoch 5/50\n",
      "0: Epoch 6/50\n",
      "0: Epoch 7/50\n",
      "0: Epoch 8/50\n",
      "0: Epoch 9/50\n",
      "0: Epoch 10/50\n",
      "0: Epoch 11/50\n",
      "0: Epoch 12/50\n",
      "0: Epoch 13/50\n",
      "0: Epoch 14/50\n",
      "0: Epoch 15/50\n",
      "0: Epoch 16/50\n",
      "0: Epoch 17/50\n",
      "0: Epoch 18/50\n",
      "0: Epoch 19/50\n",
      "0: Epoch 20/50\n",
      "0: Epoch 21/50\n",
      "0: Epoch 22/50\n",
      "0: Epoch 23/50\n",
      "0: Epoch 24/50\n",
      "0: Epoch 25/50\n",
      "0: Epoch 26/50\n",
      "0: Epoch 27/50\n",
      "0: Epoch 28/50\n",
      "0: Epoch 29/50\n",
      "0: Epoch 30/50\n",
      "0: Epoch 31/50\n",
      "0: Epoch 32/50\n",
      "0: Epoch 33/50\n",
      "0: Epoch 34/50\n",
      "0: Epoch 35/50\n",
      "0: Epoch 36/50\n",
      "0: Epoch 37/50\n",
      "0: Epoch 38/50\n",
      "0: Epoch 39/50\n",
      "0: Epoch 40/50\n",
      "0: Epoch 41/50\n",
      "0: Epoch 42/50\n",
      "0: Epoch 1/50\n",
      "0: Epoch 2/50\n",
      "0: Epoch 3/50\n",
      "0: Epoch 4/50\n",
      "0: Epoch 5/50\n",
      "0: Epoch 6/50\n",
      "0: Epoch 7/50\n",
      "0: Epoch 8/50\n",
      "0: Epoch 9/50\n",
      "0: Epoch 10/50\n",
      "0: Epoch 11/50\n",
      "0: Epoch 12/50\n",
      "0: Epoch 13/50\n",
      "0: Epoch 14/50\n",
      "0: Epoch 15/50\n",
      "0: Epoch 16/50\n",
      "0: Epoch 17/50\n",
      "0: Epoch 18/50\n",
      "0: Epoch 19/50\n",
      "0: Epoch 20/50\n",
      "0: Epoch 21/50\n",
      "0: Epoch 22/50\n",
      "0: Epoch 23/50\n",
      "0: Epoch 24/50\n",
      "0: Epoch 25/50\n",
      "0: Epoch 26/50\n",
      "0: Epoch 27/50\n",
      "0: Epoch 28/50\n",
      "0: Epoch 29/50\n",
      "0: Epoch 30/50\n",
      "0: Epoch 31/50\n",
      "0: Epoch 32/50\n",
      "0: Epoch 33/50\n"
     ]
    }
   ],
   "source": [
    "# launch the experiment\n",
    "\n",
    "result = experiment.lagom(map_fun=ablation_training_fn, experiment_type='ablation',\n",
    "                           ablation_study=iceberg_ablation, \n",
    "                           ablator='loco', \n",
    "                           name='Iceberg_ablation_study'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Step 2b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}