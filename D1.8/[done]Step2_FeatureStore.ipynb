{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg Classification Step 2: Create Feature Groups and Train/Eval datasets\n",
    "This notebook will perform the following operations:\n",
    "- Read the pre-processed data from a HopsFS dataset into a PySpark dataframe \n",
    "- Create and Feature Group \"iceberg\"\n",
    "- Create a training and test dataset with the Feature Store API\n",
    "\n",
    "This notebook is tested with the following ``configuration`` from hopsworks.\n",
    "<div>\n",
    "<img src=\"fig/step2_jupyter_config.png\" width=\"900\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>240</td><td>application_1619040920875_0273</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hopsworks-2.novalocal:8088/proxy/application_1619040920875_0273/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworks-2.novalocal:8042/node/containerlogs/container_e05_1619040920875_0273_01_000001/ExtremeEarth__tianzew0\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "No module named 'hsfs.rule'\n",
      "Traceback (most recent call last):\n",
      "ModuleNotFoundError: No module named 'hsfs.rule'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hops import hdfs\n",
    "from hops import pandas_helper as pd\n",
    "import hsfs\n",
    "from hsfs.rule import Rule\n",
    "\n",
    "# SparkSession available as 'spark'\n",
    "print(\n",
    "    f\"-----------------------------------------------\\n\" \\\n",
    "    f\"This notebook is tested with:\\n\" \\\n",
    "    f\"  - Hopsworks 2.2.0\\n\" \\\n",
    "    f\"  - Spark {spark.version}.\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define relevant paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds_path: hdfs://rpc.namenode.service.consul:8020/Projects/ExtremeEarth/eodata/train.json\n",
      "train_preprocessed_all_ds_path: hdfs://rpc.namenode.service.consul:8020/Projects/ExtremeEarth/eodata/train_preprocessed_all.json\n",
      "train_preprocessed_ds_path: hdfs://rpc.namenode.service.consul:8020/Projects/ExtremeEarth/eodata/train_preprocessed.json\n",
      "test_preprocessed_ds_path: hdfs://rpc.namenode.service.consul:8020/Projects/ExtremeEarth/eodata/test_preprocessed.json"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = 'eodata'\n",
    "train_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER,'train.json')\n",
    "train_preprocessed_all_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER, 'train_preprocessed_all.json')\n",
    "train_preprocessed_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER, 'train_preprocessed.json')\n",
    "test_preprocessed_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER, 'test_preprocessed.json')\n",
    "\n",
    "print(\"train_ds_path:\", train_ds_path)\n",
    "print(\"train_preprocessed_all_ds_path:\", train_preprocessed_all_ds_path)\n",
    "print(\"train_preprocessed_ds_path:\", train_preprocessed_ds_path)\n",
    "print(\"test_preprocessed_ds_path:\", test_preprocessed_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read raw train with spark and insert into feature store\n",
    "train_preprocessed_all_df = spark.read.format('json').load(train_preprocessed_all_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- band_1: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- band_2: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- band_avg: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- inc_angle: string (nullable = true)\n",
      " |-- is_iceberg: long (nullable = true)"
     ]
    }
   ],
   "source": [
    "train_preprocessed_all_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------+---------+----------+\n",
      "|              band_1|              band_2|            band_avg|      id|inc_angle|is_iceberg|\n",
      "+--------------------+--------------------+--------------------+--------+---------+----------+\n",
      "|[-27.878361, -27....|[-27.154118, -29....|[-27.5162395, -28...|dfd5f913|  43.9239|         0|\n",
      "|[-12.242375, -14....|[-31.506321, -27....|[-21.874348, -21....|e25388fd|  38.1562|         0|\n",
      "|[-24.603676, -24....|[-24.870956, -24....|[-24.737316, -24....|58b2aaa0|  45.2859|         1|\n",
      "|[-22.454607, -23....|[-27.889421, -27....|[-25.172014, -25....|4cfc3a18|  43.8306|         0|\n",
      "|[-26.006956, -23....|[-27.206915, -30....|[-26.6069355, -26...|271f93f4|  35.6256|         0|\n",
      "+--------------------+--------------------+--------------------+--------+---------+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "train_preprocessed_all_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "conn = hsfs.connection()\n",
    "fs = conn.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create feature expectations with validation rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "'FeatureStore' object has no attribute 'create_expectation'\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'FeatureStore' object has no attribute 'create_expectation'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expectation_id = fs.create_expectation(\"icebergs_id\",  \n",
    "                                       description=\"validate inc_angle feature values\",\n",
    "                                       features=[\"id\"], \n",
    "                                       rules=[Rule(name=\"HAS_DATATYPE\", level=\"ERROR\",accepted_type=\"Null\", max=0)])\n",
    "expectation_id.save()\n",
    "\n",
    "expectation_label = fs.create_expectation(\"is_iceberg\",\n",
    "                                         features=[\"is_iceberg\"], \n",
    "                                         description=\"validate is_iceberg label values\",\n",
    "                                         rules=[Rule(name=\"HAS_DATATYPE\", level=\"ERROR\", accepted_type=\"Integral\", min=1), \n",
    "                                                Rule(name=\"HAS_MAX\", level=\"ERROR\", min=1, max=1), \n",
    "                                                Rule(name=\"HAS_MIN\", level=\"ERROR\", min=0, max=0)])\n",
    "\n",
    "expectation_label.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and save features to the Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'expectation_id' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'expectation_id' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "icebergs_fg = fs.create_feature_group(\n",
    "    \"iceberg\",\n",
    "    time_travel_format=None,\n",
    "    statistics_config=hsfs.statistics_config.StatisticsConfig(enabled=False, correlations=False, histograms=False, columns=[]),\n",
    "    expectations=[expectation_id, expectation_label],\n",
    "    validation_type=\"STRICT\",\n",
    "    description=\"Training dataset in Feature Store for iceberg classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'icebergs_fg' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'icebergs_fg' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "icebergs_fg.save(train_preprocessed_all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retreving validation results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'icebergs_fg' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'icebergs_fg' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "[print(json.dumps(validation.to_dict(), indent=2)) for validation in icebergs_fg.get_validations()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split\n",
    "Now that preprocessing is done, let's split the feature data into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAND_SEED = 42\n",
    "TRAIN_SIZE = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'icebergs_fg' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'icebergs_fg' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "icebergs_fg.read().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'icebergs_fg' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'icebergs_fg' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read feature group data, split into train/test and export in tfrecords\n",
    "icebergs_train_df, icebergs_test_df = icebergs_fg.read().randomSplit([TRAIN_SIZE, 1-TRAIN_SIZE], RAND_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'icebergs_train_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'icebergs_train_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training dataset contains {} records\".format(icebergs_train_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'icebergs_test_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'icebergs_test_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing dataset contains {} records\".format(icebergs_test_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'icebergs_train_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'icebergs_train_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a traiing dataset of TFRecord\n",
    "icebergs_train_td = fs.create_training_dataset(\n",
    "    \"train_tfrecords_iceberg_classification_dataset\",\n",
    "    statistics_config=hsfs.statistics_config.StatisticsConfig(enabled=False, correlations=False, histograms=False, columns=[]),\n",
    "    data_format = \"tfrecords\"\n",
    ").save(icebergs_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'icebergs_test_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'icebergs_test_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a traiing dataset of TFRecord\n",
    "icebergs_test_td = fs.create_training_dataset(\n",
    "    \"test_tfrecords_iceberg_classification_dataset\",\n",
    "    statistics_config=hsfs.statistics_config.StatisticsConfig(enabled=False, correlations=False, histograms=False, columns=[]),\n",
    "    data_format = \"tfrecords\"\n",
    ").save(icebergs_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}